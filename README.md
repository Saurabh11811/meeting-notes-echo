# Project ECHO â€“ Meeting Notes, Your Way

> **Crossâ€‘platform â€¢ Customizable â€¢ Controllable meeting summaries**

Project **ECHO** (Executive Calls, Highlights & Outcomes) turns meeting recordings into **readyâ€‘toâ€‘share minutes**.  
Unlike blackâ€‘box tools, ECHO is built to be **auditable**, **backendâ€‘agnostic**, and **privacyâ€‘aware**.

---

## âœ¨ Why ECHO
- **Crossâ€‘platform**: Works with Teams/Stream (URL) *and* raw media files (MP4/MP3/etc.).
- **Configurable**: Prompts live in files; YAML config is explicit and versionable.
- **Backendâ€‘flexible**: Local (Ollama), Azure OpenAI, Dify (native App API), or your choice.
- **Privacyâ€‘first**: Onâ€‘device summarization path (Ollama) with no cloud dependency.
- **Portable outputs**: `*_transcript.txt`, `*_summary.txt`, and `.eml` email drafts.

---

## ğŸ§± Architecture (MVP)
```
Recording (File OR Teams/Stream URL)
        â”‚
        â–¼
 Transcript Capture
    â”œâ”€â”€ Use built-in transcript (if visible)
    â””â”€â”€ ASR (Whisper: faster-whisper or HF pipeline)
        â”‚
        â–¼
 Summarization (Either of Backend)
    â”œâ”€â”€ Local (Ollama)
    â”œâ”€â”€ Azure OpenAI
    â””â”€â”€ Dify (native App API)
        â”‚
        â–¼
      Outputs
    â”œâ”€â”€ <name>_transcript.txt
    â”œâ”€â”€ <name>_summary.txt
    â””â”€â”€ <name>.eml
```

---

## ğŸ“¦ Repo Layout
```
.
â”œâ”€ echo.py                # One-page Gradio UI (file OR URL)
â”œâ”€ chat_helper.py         # Teams/Stream transcript capture (browser)
â”œâ”€ calls_helper.py        # Local ASR (Whisper) for media files
â”œâ”€ helpers_local.py       # Summaries via Ollama (local)
â”œâ”€ helpers_azure.py       # Summaries via Azure OpenAI
â”œâ”€ helpers_dify.py        # Summaries via Dify (native App API)
â”œâ”€ prompts.py             # Centralized prompt loader (file/inline/default)
â”œâ”€ config.yaml            # Auto-created on first run; editable
â””â”€ out/                   # Outputs (transcripts, summaries, .eml)
```

---

## ğŸš€ Quick Start

### 1) Requirements
- **Python 3.12+** (3.10+ works; tested on macOS)
- `ffmpeg` installed and on PATH
- Optional: **Ollama** for local LLM
- Optional: Playwright (Chromium) for URL capture

### 2) Install deps
```bash
pip install -r requirements.txt
python -m playwright install chromium
```

> If you donâ€™t use URLs (only local files), you can skip Playwright.

### 3) Run the UI
```bash
python echo.py
```
Then open the browser page it prints (usually `http://127.0.0.1:7860`).

---

## ğŸ§­ Using the App
- **Inputs**: Upload a file **OR** paste a Teams/Stream URL (strict OR).
- **Setup Panel (left)**:
  - Auto environment checks (ffmpeg, Playwright, MPS, Ollama).
  - Choose **summary backend**: Local / Dify / Azure.
  - Enter backend credentials (Azure endpoint, Dify key, etc.).
  - **Prompt**: Provide a **file path** _or_ inline text (falls back to default).
  - Output directory & save toggles (transcript / summary / `.eml`).
  - **Test All Connections** button verifies Local/Dify/Azure in one click.
- **Outputs**: Full **Summary**, **Transcript**, and a **Status** panel with timings & saved paths.

---

## âš™ï¸ Configuration (`config.yaml`)
Created automatically on first run. Example:
```yaml
summary:
  backend: local              # local | azure | dify
  model: llama3:latest        # legacy; used by local if not set under backends.local
  max_chars: 0                # 0 = no trim
  prompt_file: ""             # optional path to a .txt prompt
  prompt_text: ""             # inline fallback if file missing/empty

backends:
  local:
    model: llama3:latest
  azure:
    endpoint: "https://<resource>.openai.azure.com"
    api_key: "<your-key>"
    api_version: "2024-02-15-preview"
    deployment: "gpt-4o"     # deployment name (not model id)
  dify:
    base_url: "https://api.dify.ai"
    api_key: "<your-dify-app-key>"

output:
  dir: "./out"
  save_raw_transcript: true
  save_summary: true
  save_email_draft: true
  retention_days: 14

asr:
  model_size: "small"         # tiny | base | small | medium | large-v3
  language: "en"              # 'auto' to detect
  vad: false
  force_hf: false             # check this to disable faster-whisper
  chunk_length_s: 30
  stride_length_s: 5
  concurrency: 1
```

**Prompt resolution order** (via `prompts.py`):
1. If `summary.prompt_file` is set and readable â†’ use its content.
2. Else if `summary.prompt_text` is set â†’ use that.
3. Else â†’ a **default executive-summary** prompt ships with the repo.

---

## ğŸ”Œ Backends

### Local (Ollama)
- Set model under `backends.local.model` (e.g., `llama3:latest`).
- The UI checks if Ollama is installed and if model is present.
- Great for **private** / **offline** summarization.

### Azure OpenAI
- Provide **endpoint**, **api_key**, **api_version**, and **deployment** (name).
- We call the **Chat Completions** API with your deployment.

### Dify (Native App API)
- Provide **base_url** and **app API key** (not OpenAI-compatible key).
- We send the transcript via `/v1/chat-messages` in blocking mode.
- If your Dify app expects a â€œsystem promptâ€, we **prepend** it to the user message.

---

## ğŸ§  Transcript Capture (URL)
- We **prefer built-in Teams transcript** when visible.
- If the meeting is DRMâ€‘protected for nonâ€‘owners, we **donâ€™t** bypass it.  
  The scraper only reads safe transcripts/captions via DOM or payload sniffing.
- If transcript is not visible, switch to **owner mode** or use **file + ASR**.

---

## ğŸ™ï¸ ASR (Local Files)
- Uses **faster-whisper** by default (CTranslate2).  
  - On Macs, CPU int8 is typically fastest; MPS is not used by fasterâ€‘whisper.
- **HF Whisper pipeline** is the fallback (MPS/CPU/GPU).  
- Toggle **â€œForce HFâ€** if fasterâ€‘whisper isnâ€™t available or misbehaves.

---

## ğŸ§ª Health Checks
Click **Test All Connections** in the UI to verify:
- Local (Ollama model)
- Dify (native App API)
- Azure OpenAI (deployment reachable)

---

## ğŸ› ï¸ Troubleshooting
- **Playwright error / no Chromium**: run `python -m playwright install chromium`.
- **No transcript from URL**: Ensure youâ€™re **owner** or transcript is visible to you.
- **ASR: â€œno audio streamâ€**: The file may not have an audio track. Check with `ffprobe`.
- **Azure â€œdeployment not foundâ€**: The **deployment name** is required (not just model id).
- **Ollama model missing**: Run `ollama pull llama3:latest` (or your model tag).

---

## ğŸ—ºï¸ Wishlist and Next steps
- PII redaction + retention policies
- Zoom/Webex/GMeet capture
- Structured MoM templates (RACI, owners/dates)
- Red-team/privacy validation

---

## ğŸ”’ Compliance & Privacy
- The URL capture flow **never attempts to bypass DRM** or protected streams.
- Local mode (Ollama) ensures content **stays on device**.
- You control what to persist via `config.yaml` (transcripts, summaries, email drafts).

